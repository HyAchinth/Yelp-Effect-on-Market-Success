{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3955f5e2-af76-4d97-88f6-a3a54d9fcccc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "  gelati kaldi beiler whopper paesano insomnia kolaches matzo ube caje\n",
      "--------------------------------------------------\n",
      "Topic #1:\n",
      "  shelly bimbo rasoi uzbek enforcing bono teller giovanni nikki alisha\n",
      "--------------------------------------------------\n",
      "Topic #2:\n",
      "  ed dc dv8 luna manger mccoy lemp arby anyplace tbbc\n",
      "--------------------------------------------------\n",
      "Topic #3:\n",
      "  omakase halo hinoki siu ming hunan washoe iztaccihuatl yummm delilah\n",
      "--------------------------------------------------\n",
      "Topic #4:\n",
      "  sampan wegman finney twistee nibbled coffeeshop riverbench thy elsie evos\n",
      "--------------------------------------------------\n",
      "Topic #5:\n",
      "  vault naughty alway spectacle phuong bobacup sardi josephine galaxy distrito\n",
      "--------------------------------------------------\n",
      "Topic #6:\n",
      "  certificate goo lechon taffy snacking gabby culver nova layover pierogis\n",
      "--------------------------------------------------\n",
      "Topic #7:\n",
      "  gay woody rous marte greg verti fil honky tonk molly\n",
      "--------------------------------------------------\n",
      "Topic #8:\n",
      "  food good place like one great time get chicken restaurant\n",
      "--------------------------------------------------\n",
      "Topic #9:\n",
      "  hush bestie grubhub waterfront puerto louie chimichanga cv poboys pupusas\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Load TF-IDF matrix\n",
    "tfidf_df = pd.read_parquet(\"tfidf_yelp_reviews.parquet\")\n",
    "tfidf_matrix = tfidf_df.values\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 10  # You can tune this\n",
    "\n",
    "# Initialize and fit LDA model\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=num_topics, \n",
    "    random_state=42, \n",
    "    learning_method='batch',  # You can change to 'online' for large datasets\n",
    "    max_iter=200  # Increase if needed\n",
    ")\n",
    "W = lda_model.fit_transform(tfidf_matrix)  # Document-Topic distribution (shape: 20234 x 10)\n",
    "H = lda_model.components_  # Topic-Term matrix (shape: 10 x 20000)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = tfidf_df.columns\n",
    "n_top_words = 10\n",
    "\n",
    "# Print top words for each topic\n",
    "for topic_idx, topic_weights in enumerate(H):\n",
    "    top_word_indices = topic_weights.argsort()[::-1][:n_top_words]\n",
    "    top_words = [feature_names[i] for i in top_word_indices]\n",
    "    print(f\"Topic #{topic_idx}:\")\n",
    "    print(\"  \" + \" \".join(top_words))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Assign each document its most probable topic\n",
    "doc_topics = np.argmax(W, axis=1)\n",
    "\n",
    "# If you want to add this back to a DataFrame\n",
    "# (Assuming you have a matching index of documents)\n",
    "# original_df['dominant_topic'] = doc_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebd092d-68ca-48cb-928e-4cddd47c825e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save dominant topic for each document\n",
    "doc_topic_df = pd.DataFrame({\n",
    "    'document_index': range(len(doc_topics)),\n",
    "    'dominant_topic': doc_topics\n",
    "})\n",
    "doc_topic_df.to_parquet(\"document_topics.parquet\", index=False)\n",
    "\n",
    "# Save top words per topic\n",
    "topics_data = []\n",
    "for topic_idx, topic_weights in enumerate(H):\n",
    "    top_word_indices = topic_weights.argsort()[::-1][:n_top_words]\n",
    "    top_words = [feature_names[i] for i in top_word_indices]\n",
    "    topics_data.append({\"topic\": topic_idx, \"top_words\": \", \".join(top_words)})\n",
    "\n",
    "topics_df = pd.DataFrame(topics_data)\n",
    "topics_df.to_parquet(\"lda_topics.parquet\", index=False)\n",
    "\n",
    "# Save full topic probability distribution (W matrix)\n",
    "W_df = pd.DataFrame(W, columns=[f\"topic_{i}\" for i in range(num_topics)])\n",
    "W_df.to_parquet(\"document_topic_distribution.parquet\", index=False)\n",
    "\n",
    "print(\"Results saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-17.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-17:m127"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
