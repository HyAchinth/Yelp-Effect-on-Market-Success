{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b0701b-14d9-49be-bfca-21c3dbe2cd74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c7d100-29f2-4c3e-baee-349c32d7673d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_5806/3769299552.py:1: DtypeWarning: Columns (0,5,6,7,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_review = pd.read_csv(\"merged_review_business.csv\")\n"
     ]
    }
   ],
   "source": [
    "merged_review = pd.read_csv(\"merged_review_business.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55f5887-ad59-4239-a2f2-bd9e873d9151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5126138 entries, 0 to 5126137\n",
      "Data columns (total 26 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   Unnamed: 0    object \n",
      " 1   review_id     object \n",
      " 2   user_id       object \n",
      " 3   business_id   object \n",
      " 4   stars_x       float64\n",
      " 5   useful        object \n",
      " 6   funny         object \n",
      " 7   cool          object \n",
      " 8   text          object \n",
      " 9   date          object \n",
      " 10  year          float64\n",
      " 11  month         float64\n",
      " 12  day_of_week   float64\n",
      " 13  name          object \n",
      " 14  address       object \n",
      " 15  city          object \n",
      " 16  state         object \n",
      " 17  postal_code   object \n",
      " 18  latitude      float64\n",
      " 19  longitude     float64\n",
      " 20  stars_y       float64\n",
      " 21  review_count  float64\n",
      " 22  is_open       float64\n",
      " 23  attributes    object \n",
      " 24  categories    object \n",
      " 25  hours         object \n",
      "dtypes: float64(9), object(17)\n",
      "memory usage: 1016.8+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_review.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e4552c2-4888-454e-afec-c57368fe90fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_review[['useful', 'funny', 'cool']] = merged_review[['useful', 'funny', 'cool']].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f867691-2cf2-441b-bb5e-da79c3382cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_review['review_relelvance'] = merged_review['useful'] + merged_review['funny'] + merged_review['cool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "442cfde1-c3b7-456d-b12e-aeee5b5322d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars_x</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>review_relelvance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.126137e+06</td>\n",
       "      <td>5.126136e+06</td>\n",
       "      <td>5.126136e+06</td>\n",
       "      <td>5.126136e+06</td>\n",
       "      <td>5.126136e+06</td>\n",
       "      <td>5.126136e+06</td>\n",
       "      <td>5.126136e+06</td>\n",
       "      <td>5.126135e+06</td>\n",
       "      <td>5.126135e+06</td>\n",
       "      <td>5.126135e+06</td>\n",
       "      <td>5.126135e+06</td>\n",
       "      <td>5.126135e+06</td>\n",
       "      <td>5.126136e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.802053e+00</td>\n",
       "      <td>1.010931e+00</td>\n",
       "      <td>3.120772e-01</td>\n",
       "      <td>5.002046e-01</td>\n",
       "      <td>2.016485e+03</td>\n",
       "      <td>6.431944e+00</td>\n",
       "      <td>3.146123e+00</td>\n",
       "      <td>3.589821e+01</td>\n",
       "      <td>-8.914506e+01</td>\n",
       "      <td>3.801560e+00</td>\n",
       "      <td>4.655727e+02</td>\n",
       "      <td>8.007963e-01</td>\n",
       "      <td>1.823212e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.392969e+00</td>\n",
       "      <td>2.784273e+00</td>\n",
       "      <td>1.720867e+00</td>\n",
       "      <td>2.202156e+00</td>\n",
       "      <td>3.217365e+00</td>\n",
       "      <td>3.407204e+00</td>\n",
       "      <td>2.078064e+00</td>\n",
       "      <td>5.344874e+00</td>\n",
       "      <td>1.439362e+01</td>\n",
       "      <td>6.421224e-01</td>\n",
       "      <td>8.245235e+02</td>\n",
       "      <td>3.994016e-01</td>\n",
       "      <td>5.972363e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.595191e+01</td>\n",
       "      <td>-8.680667e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.756446e+01</td>\n",
       "      <td>-1.200837e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.015000e+03</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.996033e+01</td>\n",
       "      <td>-9.024299e+01</td>\n",
       "      <td>3.500000e+00</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.017000e+03</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.860115e+01</td>\n",
       "      <td>-8.616001e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.030000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.019000e+03</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>3.994383e+01</td>\n",
       "      <td>-7.545917e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.770000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>4.200000e+02</td>\n",
       "      <td>7.920000e+02</td>\n",
       "      <td>4.040000e+02</td>\n",
       "      <td>2.022000e+03</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>5.367920e+01</td>\n",
       "      <td>-7.466135e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>7.568000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.011000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stars_x        useful         funny          cool          year  \\\n",
       "count  5.126137e+06  5.126136e+06  5.126136e+06  5.126136e+06  5.126136e+06   \n",
       "mean   3.802053e+00  1.010931e+00  3.120772e-01  5.002046e-01  2.016485e+03   \n",
       "std    1.392969e+00  2.784273e+00  1.720867e+00  2.202156e+00  3.217365e+00   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  3.595191e+01   \n",
       "25%    3.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  2.015000e+03   \n",
       "50%    4.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  2.017000e+03   \n",
       "75%    5.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00  2.019000e+03   \n",
       "max    5.000000e+00  4.200000e+02  7.920000e+02  4.040000e+02  2.022000e+03   \n",
       "\n",
       "              month   day_of_week      latitude     longitude       stars_y  \\\n",
       "count  5.126136e+06  5.126136e+06  5.126135e+06  5.126135e+06  5.126135e+06   \n",
       "mean   6.431944e+00  3.146123e+00  3.589821e+01 -8.914506e+01  3.801560e+00   \n",
       "std    3.407204e+00  2.078064e+00  5.344874e+00  1.439362e+01  6.421224e-01   \n",
       "min   -8.680667e+01  0.000000e+00  2.756446e+01 -1.200837e+02  1.000000e+00   \n",
       "25%    3.000000e+00  1.000000e+00  2.996033e+01 -9.024299e+01  3.500000e+00   \n",
       "50%    6.000000e+00  3.000000e+00  3.860115e+01 -8.616001e+01  4.000000e+00   \n",
       "75%    9.000000e+00  5.000000e+00  3.994383e+01 -7.545917e+01  4.000000e+00   \n",
       "max    1.200000e+01  6.000000e+00  5.367920e+01 -7.466135e+01  5.000000e+00   \n",
       "\n",
       "       review_count       is_open  review_relelvance  \n",
       "count  5.126135e+06  5.126135e+06       5.126136e+06  \n",
       "mean   4.655727e+02  8.007963e-01       1.823212e+00  \n",
       "std    8.245235e+02  3.994016e-01       5.972363e+00  \n",
       "min    5.000000e+00  0.000000e+00       0.000000e+00  \n",
       "25%    8.000000e+01  1.000000e+00       0.000000e+00  \n",
       "50%    2.030000e+02  1.000000e+00       0.000000e+00  \n",
       "75%    4.770000e+02  1.000000e+00       2.000000e+00  \n",
       "max    7.568000e+03  1.000000e+00       1.011000e+03  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_review.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c39d54aa-f7bc-42f8-ac3c-013d052285e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars_x</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "      <th>review_relelvance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
       "      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "      <td>2018-07-07 22:09:11</td>\n",
       "      <td>...</td>\n",
       "      <td>19454</td>\n",
       "      <td>40.210196</td>\n",
       "      <td>-75.223639</td>\n",
       "      <td>3.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'AcceptsInsurance': None, 'AgesAllowed': None...</td>\n",
       "      <td>['Restaurants', 'Breakfast &amp; Brunch', 'Food', ...</td>\n",
       "      <td>{'Friday': '7:30-15:0', 'Monday': '7:30-15:0',...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>VJxlBnJmCDIy8DFG0kjSow</td>\n",
       "      <td>Iaee7y6zdSB3B-kRCo4z1w</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is the second time we tried turning point...</td>\n",
       "      <td>2017-05-13 17:06:55</td>\n",
       "      <td>...</td>\n",
       "      <td>19454</td>\n",
       "      <td>40.210196</td>\n",
       "      <td>-75.223639</td>\n",
       "      <td>3.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'AcceptsInsurance': None, 'AgesAllowed': None...</td>\n",
       "      <td>['Restaurants', 'Breakfast &amp; Brunch', 'Food', ...</td>\n",
       "      <td>{'Friday': '7:30-15:0', 'Monday': '7:30-15:0',...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S6pQZQocMB1WHMjTRbt77A</td>\n",
       "      <td>ejFxLGqQcWNLdNByJlIhnQ</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The place is cute and the staff was very frien...</td>\n",
       "      <td>2017-08-08 00:58:18</td>\n",
       "      <td>...</td>\n",
       "      <td>19454</td>\n",
       "      <td>40.210196</td>\n",
       "      <td>-75.223639</td>\n",
       "      <td>3.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'AcceptsInsurance': None, 'AgesAllowed': None...</td>\n",
       "      <td>['Restaurants', 'Breakfast &amp; Brunch', 'Food', ...</td>\n",
       "      <td>{'Friday': '7:30-15:0', 'Monday': '7:30-15:0',...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>WqgTKVqWVHDHjnjEsBvUgg</td>\n",
       "      <td>f7xa0p_1V9lx53iIGN5Sug</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We came on a Saturday morning after waiting a ...</td>\n",
       "      <td>2017-11-19 02:20:23</td>\n",
       "      <td>...</td>\n",
       "      <td>19454</td>\n",
       "      <td>40.210196</td>\n",
       "      <td>-75.223639</td>\n",
       "      <td>3.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'AcceptsInsurance': None, 'AgesAllowed': None...</td>\n",
       "      <td>['Restaurants', 'Breakfast &amp; Brunch', 'Food', ...</td>\n",
       "      <td>{'Friday': '7:30-15:0', 'Monday': '7:30-15:0',...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>M0wzFFb7pefOPcxeRVbLag</td>\n",
       "      <td>dCooFVCk8M1nVaQqcfTL3Q</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mediocre at best. The decor is very nice, and ...</td>\n",
       "      <td>2017-09-09 17:49:47</td>\n",
       "      <td>...</td>\n",
       "      <td>19454</td>\n",
       "      <td>40.210196</td>\n",
       "      <td>-75.223639</td>\n",
       "      <td>3.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'AcceptsInsurance': None, 'AgesAllowed': None...</td>\n",
       "      <td>['Restaurants', 'Breakfast &amp; Brunch', 'Food', ...</td>\n",
       "      <td>{'Friday': '7:30-15:0', 'Monday': '7:30-15:0',...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0               review_id                 user_id  \\\n",
       "0          0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n",
       "1          1  VJxlBnJmCDIy8DFG0kjSow  Iaee7y6zdSB3B-kRCo4z1w   \n",
       "2          2  S6pQZQocMB1WHMjTRbt77A  ejFxLGqQcWNLdNByJlIhnQ   \n",
       "3          3  WqgTKVqWVHDHjnjEsBvUgg  f7xa0p_1V9lx53iIGN5Sug   \n",
       "4          4  M0wzFFb7pefOPcxeRVbLag  dCooFVCk8M1nVaQqcfTL3Q   \n",
       "\n",
       "              business_id  stars_x  useful  funny  cool  \\\n",
       "0  XQfwVwDr-v0ZS3_CbbE5Xw      3.0     0.0    0.0   0.0   \n",
       "1  XQfwVwDr-v0ZS3_CbbE5Xw      2.0     0.0    0.0   0.0   \n",
       "2  XQfwVwDr-v0ZS3_CbbE5Xw      4.0     2.0    0.0   1.0   \n",
       "3  XQfwVwDr-v0ZS3_CbbE5Xw      3.0     0.0    0.0   0.0   \n",
       "4  XQfwVwDr-v0ZS3_CbbE5Xw      2.0     0.0    0.0   0.0   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  If you decide to eat here, just be aware it is...  2018-07-07 22:09:11   \n",
       "1  This is the second time we tried turning point...  2017-05-13 17:06:55   \n",
       "2  The place is cute and the staff was very frien...  2017-08-08 00:58:18   \n",
       "3  We came on a Saturday morning after waiting a ...  2017-11-19 02:20:23   \n",
       "4  Mediocre at best. The decor is very nice, and ...  2017-09-09 17:49:47   \n",
       "\n",
       "   ...  postal_code   latitude  longitude stars_y review_count is_open  \\\n",
       "0  ...        19454  40.210196 -75.223639     3.0        169.0     1.0   \n",
       "1  ...        19454  40.210196 -75.223639     3.0        169.0     1.0   \n",
       "2  ...        19454  40.210196 -75.223639     3.0        169.0     1.0   \n",
       "3  ...        19454  40.210196 -75.223639     3.0        169.0     1.0   \n",
       "4  ...        19454  40.210196 -75.223639     3.0        169.0     1.0   \n",
       "\n",
       "                                          attributes  \\\n",
       "0  {'AcceptsInsurance': None, 'AgesAllowed': None...   \n",
       "1  {'AcceptsInsurance': None, 'AgesAllowed': None...   \n",
       "2  {'AcceptsInsurance': None, 'AgesAllowed': None...   \n",
       "3  {'AcceptsInsurance': None, 'AgesAllowed': None...   \n",
       "4  {'AcceptsInsurance': None, 'AgesAllowed': None...   \n",
       "\n",
       "                                          categories  \\\n",
       "0  ['Restaurants', 'Breakfast & Brunch', 'Food', ...   \n",
       "1  ['Restaurants', 'Breakfast & Brunch', 'Food', ...   \n",
       "2  ['Restaurants', 'Breakfast & Brunch', 'Food', ...   \n",
       "3  ['Restaurants', 'Breakfast & Brunch', 'Food', ...   \n",
       "4  ['Restaurants', 'Breakfast & Brunch', 'Food', ...   \n",
       "\n",
       "                                               hours  review_relelvance  \n",
       "0  {'Friday': '7:30-15:0', 'Monday': '7:30-15:0',...                0.0  \n",
       "1  {'Friday': '7:30-15:0', 'Monday': '7:30-15:0',...                0.0  \n",
       "2  {'Friday': '7:30-15:0', 'Monday': '7:30-15:0',...                3.0  \n",
       "3  {'Friday': '7:30-15:0', 'Monday': '7:30-15:0',...                0.0  \n",
       "4  {'Friday': '7:30-15:0', 'Monday': '7:30-15:0',...                0.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52474529-ab2d-4c50-9840-76c99a4d80a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m merged_review_filtered \u001b[38;5;241m=\u001b[39m \u001b[43mm1\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhours\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm1' is not defined"
     ]
    }
   ],
   "source": [
    "merged_review_filtered = m1.drop(columns=['attributes', 'hours'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74f2972c-8153-4c98-b5f5-6f351ab1bd5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars_x</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>categories</th>\n",
       "      <th>review_relelvance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>367</td>\n",
       "      <td>kUIo6GuLZGeWhUs1juO3tg</td>\n",
       "      <td>-G7Zkl1wIWBBmD0KRy_sCw</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Zaika on Grant Avenue in Northeast Philly, is ...</td>\n",
       "      <td>2017-04-28 13:32:57</td>\n",
       "      <td>...</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "      <td>19114</td>\n",
       "      <td>40.079848</td>\n",
       "      <td>-75.025080</td>\n",
       "      <td>4.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Halal', 'Pakistani', 'Restaurants', 'Indian']</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>381</td>\n",
       "      <td>wNu9HLw8ZSqmef4XlmrdNg</td>\n",
       "      <td>3QnoTcrxuafMCoTzW_AH7A</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Awesome find!! I can safely say after my first...</td>\n",
       "      <td>2017-03-24 10:55:39</td>\n",
       "      <td>...</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "      <td>19114</td>\n",
       "      <td>40.079848</td>\n",
       "      <td>-75.025080</td>\n",
       "      <td>4.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Halal', 'Pakistani', 'Restaurants', 'Indian']</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>1311</td>\n",
       "      <td>TWnIcUQQ1M_ShPnhSK-krA</td>\n",
       "      <td>iJQtNnEcrxdpaD132v26nQ</td>\n",
       "      <td>gebiRewfieSdtt17PTW6Zg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Something's Dirty - very dirty, and not very g...</td>\n",
       "      <td>2016-04-23 15:21:03</td>\n",
       "      <td>...</td>\n",
       "      <td>Santa Barbara</td>\n",
       "      <td>CA</td>\n",
       "      <td>93101</td>\n",
       "      <td>34.416984</td>\n",
       "      <td>-119.695556</td>\n",
       "      <td>3.5</td>\n",
       "      <td>488.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Steakhouses', 'Sushi Bars', 'Restaurants', '...</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>2286</td>\n",
       "      <td>6LqVOdbbUPErI1q_prX90A</td>\n",
       "      <td>AHRrG3T1gJpHvtpZ-K0G_g</td>\n",
       "      <td>EQ-TZ2eeD_E0BHuvoaeG5Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>This is a very overdue update. You'll often fi...</td>\n",
       "      <td>2015-08-24 01:03:12</td>\n",
       "      <td>...</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>IN</td>\n",
       "      <td>46203</td>\n",
       "      <td>39.759169</td>\n",
       "      <td>-86.146494</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1379.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Beer', 'Wine &amp; Spirits', 'Cafes', 'Coffee &amp; ...</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>2456</td>\n",
       "      <td>QScYcBhRnHidLBEgEPYy1Q</td>\n",
       "      <td>BHDBfx4mWOdtT-ug0CPc3w</td>\n",
       "      <td>EQ-TZ2eeD_E0BHuvoaeG5Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Oh my heavens... So pleasantly surprised to fi...</td>\n",
       "      <td>2019-05-31 19:59:28</td>\n",
       "      <td>...</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>IN</td>\n",
       "      <td>46203</td>\n",
       "      <td>39.759169</td>\n",
       "      <td>-86.146494</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1379.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Beer', 'Wine &amp; Spirits', 'Cafes', 'Coffee &amp; ...</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0               review_id                 user_id  \\\n",
       "367         367  kUIo6GuLZGeWhUs1juO3tg  -G7Zkl1wIWBBmD0KRy_sCw   \n",
       "381         381  wNu9HLw8ZSqmef4XlmrdNg  3QnoTcrxuafMCoTzW_AH7A   \n",
       "1311       1311  TWnIcUQQ1M_ShPnhSK-krA  iJQtNnEcrxdpaD132v26nQ   \n",
       "2286       2286  6LqVOdbbUPErI1q_prX90A  AHRrG3T1gJpHvtpZ-K0G_g   \n",
       "2456       2456  QScYcBhRnHidLBEgEPYy1Q  BHDBfx4mWOdtT-ug0CPc3w   \n",
       "\n",
       "                 business_id  stars_x  useful  funny  cool  \\\n",
       "367   kxX2SOes4o-D3ZQBkiMRfA      5.0    23.0   11.0  18.0   \n",
       "381   kxX2SOes4o-D3ZQBkiMRfA      5.0    24.0   13.0  23.0   \n",
       "1311  gebiRewfieSdtt17PTW6Zg      2.0    21.0   13.0   9.0   \n",
       "2286  EQ-TZ2eeD_E0BHuvoaeG5Q      5.0    18.0    3.0  16.0   \n",
       "2456  EQ-TZ2eeD_E0BHuvoaeG5Q      5.0    23.0   13.0  21.0   \n",
       "\n",
       "                                                   text                 date  \\\n",
       "367   Zaika on Grant Avenue in Northeast Philly, is ...  2017-04-28 13:32:57   \n",
       "381   Awesome find!! I can safely say after my first...  2017-03-24 10:55:39   \n",
       "1311  Something's Dirty - very dirty, and not very g...  2016-04-23 15:21:03   \n",
       "2286  This is a very overdue update. You'll often fi...  2015-08-24 01:03:12   \n",
       "2456  Oh my heavens... So pleasantly surprised to fi...  2019-05-31 19:59:28   \n",
       "\n",
       "      ...           city  state  postal_code   latitude   longitude stars_y  \\\n",
       "367   ...   Philadelphia     PA        19114  40.079848  -75.025080     4.0   \n",
       "381   ...   Philadelphia     PA        19114  40.079848  -75.025080     4.0   \n",
       "1311  ...  Santa Barbara     CA        93101  34.416984 -119.695556     3.5   \n",
       "2286  ...   Indianapolis     IN        46203  39.759169  -86.146494     4.0   \n",
       "2456  ...   Indianapolis     IN        46203  39.759169  -86.146494     4.0   \n",
       "\n",
       "     review_count is_open                                         categories  \\\n",
       "367         181.0     1.0    ['Halal', 'Pakistani', 'Restaurants', 'Indian']   \n",
       "381         181.0     1.0    ['Halal', 'Pakistani', 'Restaurants', 'Indian']   \n",
       "1311        488.0     1.0  ['Steakhouses', 'Sushi Bars', 'Restaurants', '...   \n",
       "2286       1379.0     1.0  ['Beer', 'Wine & Spirits', 'Cafes', 'Coffee & ...   \n",
       "2456       1379.0     1.0  ['Beer', 'Wine & Spirits', 'Cafes', 'Coffee & ...   \n",
       "\n",
       "      review_relelvance  \n",
       "367                52.0  \n",
       "381                60.0  \n",
       "1311               43.0  \n",
       "2286               37.0  \n",
       "2456               57.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_review_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7bb5090-b106-4b3e-9df1-b932315376f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 999)\n",
    "pd.set_option('display.colheader_justify', 'left')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71087b09-f806-43c4-9931-a8444530e535",
   "metadata": {
    "tags": []
   },
   "source": [
    "m2 = merged_review[merged_review['review_count']>7560]\n",
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dd5017a-af90-4791-9de3-26be9377b08c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20234 entries, 367 to 5126033\n",
      "Data columns (total 25 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         20234 non-null  object \n",
      " 1   review_id          20234 non-null  object \n",
      " 2   user_id            20234 non-null  object \n",
      " 3   business_id        20234 non-null  object \n",
      " 4   stars_x            20234 non-null  float64\n",
      " 5   useful             20234 non-null  float64\n",
      " 6   funny              20234 non-null  float64\n",
      " 7   cool               20234 non-null  float64\n",
      " 8   text               20234 non-null  object \n",
      " 9   date               20234 non-null  object \n",
      " 10  year               20234 non-null  float64\n",
      " 11  month              20234 non-null  float64\n",
      " 12  day_of_week        20234 non-null  float64\n",
      " 13  name               20234 non-null  object \n",
      " 14  address            20128 non-null  object \n",
      " 15  city               20234 non-null  object \n",
      " 16  state              20234 non-null  object \n",
      " 17  postal_code        20234 non-null  object \n",
      " 18  latitude           20234 non-null  float64\n",
      " 19  longitude          20234 non-null  float64\n",
      " 20  stars_y            20234 non-null  float64\n",
      " 21  review_count       20234 non-null  float64\n",
      " 22  is_open            20234 non-null  float64\n",
      " 23  categories         20234 non-null  object \n",
      " 24  review_relelvance  20234 non-null  float64\n",
      "dtypes: float64(13), object(12)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "m1 = merged_review_filtered[merged_review_filtered['review_relelvance']>32]\n",
    "m1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43034d60-9b58-48b8-a12c-c305f2e95239",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "(\"Could not convert '2392064' with type str: tried to convert to int64\", 'Conversion failed for column Unnamed: 0 with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmerged_review_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiltered_review.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:2889\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2890\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2898\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    409\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 411\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parquet.py:159\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     from_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m--> 159\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfrom_pandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    162\u001b[0m     path,\n\u001b[1;32m    163\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m     is_dir\u001b[38;5;241m=\u001b[39mpartition_cols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    167\u001b[0m )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(path_or_handle, io\u001b[38;5;241m.\u001b[39mBufferedWriter)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(path_or_handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle\u001b[38;5;241m.\u001b[39mname, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m))\n\u001b[1;32m    172\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/table.pxi:3480\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:622\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, maybe_fut \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_fut, futures\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m--> 622\u001b[0m             arrays[i] \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_fut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m types \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:596\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    592\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    593\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    594\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field_nullable \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mnull_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was non-nullable but pandas column \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    599\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m null values\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(field),\n\u001b[1;32m    600\u001b[0m                                                  result\u001b[38;5;241m.\u001b[39mnull_count))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:590\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    587\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 590\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    592\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    593\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    594\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/array.pxi:313\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/array.pxi:83\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: (\"Could not convert '2392064' with type str: tried to convert to int64\", 'Conversion failed for column Unnamed: 0 with type object')"
     ]
    }
   ],
   "source": [
    "m1.to_parquet(\"filtered_review.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47d71ce5-7ef4-4123-886c-d99e164bc010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5126138 entries, 0 to 5126137\n",
      "Data columns (total 25 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Unnamed: 0         object \n",
      " 1   review_id          object \n",
      " 2   user_id            object \n",
      " 3   business_id        object \n",
      " 4   stars_x            float64\n",
      " 5   useful             float64\n",
      " 6   funny              float64\n",
      " 7   cool               float64\n",
      " 8   text               object \n",
      " 9   date               object \n",
      " 10  year               float64\n",
      " 11  month              float64\n",
      " 12  day_of_week        float64\n",
      " 13  name               object \n",
      " 14  address            object \n",
      " 15  city               object \n",
      " 16  state              object \n",
      " 17  postal_code        object \n",
      " 18  latitude           float64\n",
      " 19  longitude          float64\n",
      " 20  stars_y            float64\n",
      " 21  review_count       float64\n",
      " 22  is_open            float64\n",
      " 23  categories         object \n",
      " 24  review_relelvance  float64\n",
      "dtypes: float64(13), object(12)\n",
      "memory usage: 977.7+ MB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7b70bbd-0345-4dd8-a0d1-67d5b6e0284b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # Correct way to download tokenizer\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b84a11bd-45e0-4a83-af76-b9655b9c0c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "367   Zaika on Grant Avenue in Northeast Philly, is ...   \n",
      "381   Awesome find!! I can safely say after my first...   \n",
      "1311  Something's Dirty - very dirty, and not very g...   \n",
      "2286  This is a very overdue update. You'll often fi...   \n",
      "2456  Oh my heavens... So pleasantly surprised to fi...   \n",
      "\n",
      "                                           cleaned_text  \n",
      "367   zaika grant avenue northeast philly one best r...  \n",
      "381   awesome find safely say first experience tasti...  \n",
      "1311  something dirty dirty good know three year ago...  \n",
      "2286  overdue update often find upcycled mechanic sh...  \n",
      "2456  oh heaven pleasantly surprised find amazing br...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def cleanup_text(text):\n",
    "    if pd.isna(text):  \n",
    "        return \"\"\n",
    "    \n",
    "    \n",
    "    tokenized_text = word_tokenize(text.lower())  \n",
    "\n",
    "    \n",
    "    tokenized_text = [word for word in tokenized_text if word.isalnum()]\n",
    "\n",
    "    \n",
    "    tokenized_text = [word for word in tokenized_text if word not in stop_words]\n",
    "\n",
    "    \n",
    "    tokenized_text = [lemmatizer.lemmatize(word) for word in tokenized_text]\n",
    "\n",
    "    return \" \".join(tokenized_text)  \n",
    "\n",
    "\n",
    "m1[\"cleaned_text\"] = m1[\"text\"].apply(cleanup_text)\n",
    "\n",
    "\n",
    "m1.to_csv(\"cleaned_yelp_reviews.csv\", index=False)\n",
    "\n",
    "\n",
    "print(m1[[\"text\", \"cleaned_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d916294f-c10d-4efe-888e-9fbea31c8f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m1 = pd.read_csv(\"cleaned_yelp_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffa6913d-6860-4159-935e-bd094c90f797",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars_x</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>categories</th>\n",
       "      <th>review_relelvance</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>367</td>\n",
       "      <td>kUIo6GuLZGeWhUs1juO3tg</td>\n",
       "      <td>-G7Zkl1wIWBBmD0KRy_sCw</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Zaika on Grant Avenue in Northeast Philly, is ...</td>\n",
       "      <td>2017-04-28 13:32:57</td>\n",
       "      <td>...</td>\n",
       "      <td>PA</td>\n",
       "      <td>19114</td>\n",
       "      <td>40.079848</td>\n",
       "      <td>-75.025080</td>\n",
       "      <td>4.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Halal', 'Pakistani', 'Restaurants', 'Indian']</td>\n",
       "      <td>52.0</td>\n",
       "      <td>zaika grant avenue northeast philly one best r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>381</td>\n",
       "      <td>wNu9HLw8ZSqmef4XlmrdNg</td>\n",
       "      <td>3QnoTcrxuafMCoTzW_AH7A</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Awesome find!! I can safely say after my first...</td>\n",
       "      <td>2017-03-24 10:55:39</td>\n",
       "      <td>...</td>\n",
       "      <td>PA</td>\n",
       "      <td>19114</td>\n",
       "      <td>40.079848</td>\n",
       "      <td>-75.025080</td>\n",
       "      <td>4.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Halal', 'Pakistani', 'Restaurants', 'Indian']</td>\n",
       "      <td>60.0</td>\n",
       "      <td>awesome find safely say first experience tasti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1311</td>\n",
       "      <td>TWnIcUQQ1M_ShPnhSK-krA</td>\n",
       "      <td>iJQtNnEcrxdpaD132v26nQ</td>\n",
       "      <td>gebiRewfieSdtt17PTW6Zg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Something's Dirty - very dirty, and not very g...</td>\n",
       "      <td>2016-04-23 15:21:03</td>\n",
       "      <td>...</td>\n",
       "      <td>CA</td>\n",
       "      <td>93101</td>\n",
       "      <td>34.416984</td>\n",
       "      <td>-119.695556</td>\n",
       "      <td>3.5</td>\n",
       "      <td>488.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Steakhouses', 'Sushi Bars', 'Restaurants', '...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>something dirty dirty good know three year ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2286</td>\n",
       "      <td>6LqVOdbbUPErI1q_prX90A</td>\n",
       "      <td>AHRrG3T1gJpHvtpZ-K0G_g</td>\n",
       "      <td>EQ-TZ2eeD_E0BHuvoaeG5Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>This is a very overdue update. You'll often fi...</td>\n",
       "      <td>2015-08-24 01:03:12</td>\n",
       "      <td>...</td>\n",
       "      <td>IN</td>\n",
       "      <td>46203</td>\n",
       "      <td>39.759169</td>\n",
       "      <td>-86.146494</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1379.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Beer', 'Wine &amp; Spirits', 'Cafes', 'Coffee &amp; ...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>overdue update often find upcycled mechanic sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2456</td>\n",
       "      <td>QScYcBhRnHidLBEgEPYy1Q</td>\n",
       "      <td>BHDBfx4mWOdtT-ug0CPc3w</td>\n",
       "      <td>EQ-TZ2eeD_E0BHuvoaeG5Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Oh my heavens... So pleasantly surprised to fi...</td>\n",
       "      <td>2019-05-31 19:59:28</td>\n",
       "      <td>...</td>\n",
       "      <td>IN</td>\n",
       "      <td>46203</td>\n",
       "      <td>39.759169</td>\n",
       "      <td>-86.146494</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1379.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Beer', 'Wine &amp; Spirits', 'Cafes', 'Coffee &amp; ...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>oh heaven pleasantly surprised find amazing br...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               review_id                 user_id  \\\n",
       "0         367  kUIo6GuLZGeWhUs1juO3tg  -G7Zkl1wIWBBmD0KRy_sCw   \n",
       "1         381  wNu9HLw8ZSqmef4XlmrdNg  3QnoTcrxuafMCoTzW_AH7A   \n",
       "2        1311  TWnIcUQQ1M_ShPnhSK-krA  iJQtNnEcrxdpaD132v26nQ   \n",
       "3        2286  6LqVOdbbUPErI1q_prX90A  AHRrG3T1gJpHvtpZ-K0G_g   \n",
       "4        2456  QScYcBhRnHidLBEgEPYy1Q  BHDBfx4mWOdtT-ug0CPc3w   \n",
       "\n",
       "              business_id  stars_x  useful  funny  cool  \\\n",
       "0  kxX2SOes4o-D3ZQBkiMRfA      5.0    23.0   11.0  18.0   \n",
       "1  kxX2SOes4o-D3ZQBkiMRfA      5.0    24.0   13.0  23.0   \n",
       "2  gebiRewfieSdtt17PTW6Zg      2.0    21.0   13.0   9.0   \n",
       "3  EQ-TZ2eeD_E0BHuvoaeG5Q      5.0    18.0    3.0  16.0   \n",
       "4  EQ-TZ2eeD_E0BHuvoaeG5Q      5.0    23.0   13.0  21.0   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  Zaika on Grant Avenue in Northeast Philly, is ...  2017-04-28 13:32:57   \n",
       "1  Awesome find!! I can safely say after my first...  2017-03-24 10:55:39   \n",
       "2  Something's Dirty - very dirty, and not very g...  2016-04-23 15:21:03   \n",
       "3  This is a very overdue update. You'll often fi...  2015-08-24 01:03:12   \n",
       "4  Oh my heavens... So pleasantly surprised to fi...  2019-05-31 19:59:28   \n",
       "\n",
       "   ...  state  postal_code   latitude   longitude stars_y review_count  \\\n",
       "0  ...     PA        19114  40.079848  -75.025080     4.0        181.0   \n",
       "1  ...     PA        19114  40.079848  -75.025080     4.0        181.0   \n",
       "2  ...     CA        93101  34.416984 -119.695556     3.5        488.0   \n",
       "3  ...     IN        46203  39.759169  -86.146494     4.0       1379.0   \n",
       "4  ...     IN        46203  39.759169  -86.146494     4.0       1379.0   \n",
       "\n",
       "  is_open                                         categories  \\\n",
       "0     1.0    ['Halal', 'Pakistani', 'Restaurants', 'Indian']   \n",
       "1     1.0    ['Halal', 'Pakistani', 'Restaurants', 'Indian']   \n",
       "2     1.0  ['Steakhouses', 'Sushi Bars', 'Restaurants', '...   \n",
       "3     1.0  ['Beer', 'Wine & Spirits', 'Cafes', 'Coffee & ...   \n",
       "4     1.0  ['Beer', 'Wine & Spirits', 'Cafes', 'Coffee & ...   \n",
       "\n",
       "   review_relelvance                                       cleaned_text  \n",
       "0               52.0  zaika grant avenue northeast philly one best r...  \n",
       "1               60.0  awesome find safely say first experience tasti...  \n",
       "2               43.0  something dirty dirty good know three year ago...  \n",
       "3               37.0  overdue update often find upcycled mechanic sh...  \n",
       "4               57.0  oh heaven pleasantly surprised find amazing br...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d83d73d0-822b-4eb9-a8e4-0b8cb3a43217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
      "Downloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Successfully installed gensim-4.3.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07b1925e-4d13-4902-a302-a480c9c932a3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===-----------------------------------------------] 6.2% 23.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========------------------------------------------] 17.5% 65.9/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============-------------------------------------] 27.6% 103.7/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================-------------------------------] 38.2% 143.8/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================--------------------------] 48.9% 183.8/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================---------------------] 59.6% 224.0/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================----------------] 69.6% 261.9/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================----------] 80.2% 301.7/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================-----] 91.8% 345.2/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "pretrained_model = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e559391-62d2-4714-931c-d6e07f832378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "vector_size = pretrained_model.vector_size  \n",
    "\n",
    "\n",
    "texts = m1[\"cleaned_text\"].astype(str).tolist()\n",
    "\n",
    "\n",
    "def get_sentence_embedding(sentence, model, vector_size):\n",
    "    words = sentence.split()  \n",
    "    embeddings = [model[word] for word in words if word in model]  \n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)  \n",
    "    else:\n",
    "        return np.zeros(vector_size)  \n",
    "\n",
    "\n",
    "glove_vectors = np.array([get_sentence_embedding(text, pretrained_model, vector_size) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ad426cb-09ea-45f6-a51a-1c543f422829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_model = TfidfVectorizer(max_features=20000)\n",
    "\n",
    "\n",
    "tfidf_matrix = tfidf_model.fit_transform(m1[\"cleaned_text\"])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_model.get_feature_names_out())\n",
    "\n",
    "\n",
    "tfidf_df.to_parquet(\"tfidf_yelp_reviews.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "946135ca-1397-4642-a62f-5140bdb29de3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>1000th</th>\n",
       "      <th>100th</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>10a</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zorba</th>\n",
       "      <th>zozo</th>\n",
       "      <th>zsa</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zudars</th>\n",
       "      <th>zukku</th>\n",
       "      <th>zuppa</th>\n",
       "      <th>zydeco</th>\n",
       "      <th>étouffée</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20229</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20230</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20231</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20232</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20233</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20234 rows × 20000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        10  100  1000  1000th  100th  101  102  104  105  10a  ...  zoom  \\\n",
       "0      0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "1      0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "2      0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "3      0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "4      0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "...    ...  ...   ...     ...    ...  ...  ...  ...  ...  ...  ...   ...   \n",
       "20229  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "20230  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "20231  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "20232  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "20233  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "\n",
       "       zorba  zozo  zsa  zucchini  zudars  zukku  zuppa  zydeco  étouffée  \n",
       "0        0.0   0.0  0.0       0.0     0.0    0.0    0.0     0.0       0.0  \n",
       "1        0.0   0.0  0.0       0.0     0.0    0.0    0.0     0.0       0.0  \n",
       "2        0.0   0.0  0.0       0.0     0.0    0.0    0.0     0.0       0.0  \n",
       "3        0.0   0.0  0.0       0.0     0.0    0.0    0.0     0.0       0.0  \n",
       "4        0.0   0.0  0.0       0.0     0.0    0.0    0.0     0.0       0.0  \n",
       "...      ...   ...  ...       ...     ...    ...    ...     ...       ...  \n",
       "20229    0.0   0.0  0.0       0.0     0.0    0.0    0.0     0.0       0.0  \n",
       "20230    0.0   0.0  0.0       0.0     0.0    0.0    0.0     0.0       0.0  \n",
       "20231    0.0   0.0  0.0       0.0     0.0    0.0    0.0     0.0       0.0  \n",
       "20232    0.0   0.0  0.0       0.0     0.0    0.0    0.0     0.0       0.0  \n",
       "20233    0.0   0.0  0.0       0.0     0.0    0.0    0.0     0.0       0.0  \n",
       "\n",
       "[20234 rows x 20000 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba58844e-0ec8-4a20-b7c2-d1984473f164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.read_parquet(\"tfidf_yelp_reviews.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82f397f2-64b3-427d-bf25-eea8fe5e0700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Cosine Similarity (First 5x5):\n",
      " [[1.         0.12961029 0.04166746 0.02488864 0.03641397]\n",
      " [0.12961029 1.         0.03523891 0.00560977 0.07885907]\n",
      " [0.04166746 0.03523891 1.         0.         0.03130077]\n",
      " [0.02488864 0.00560977 0.         1.         0.03357616]\n",
      " [0.03641397 0.07885907 0.03130077 0.03357616 1.        ]]\n",
      "\n",
      "GloVe Cosine Similarity (First 5x5):\n",
      " [[1.0000001  0.9171871  0.8556925  0.87240034 0.89136493]\n",
      " [0.9171871  0.9999994  0.9145846  0.85895234 0.92410916]\n",
      " [0.8556925  0.9145846  1.0000002  0.82690585 0.8879625 ]\n",
      " [0.87240034 0.85895234 0.82690585 1.0000001  0.8676633 ]\n",
      " [0.89136493 0.92410916 0.8879625  0.8676633  1.0000001 ]]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tfidf_cosine_sim = cosine_similarity(tfidf_df)\n",
    "\n",
    "\n",
    "glove_cosine_sim = cosine_similarity(glove_vectors)\n",
    "\n",
    "\n",
    "print(\"TF-IDF Cosine Similarity (First 5x5):\\n\", tfidf_cosine_sim[:5, :5])\n",
    "print(\"\\nGloVe Cosine Similarity (First 5x5):\\n\", glove_cosine_sim[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de49ca4e-44d6-4738-b64f-9ad2ce0379be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "glove_model = api.load('glove-wiki-gigaword-300')  \n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=20000)  \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(m1[\"cleaned_text\"])  \n",
    "tfidf_vocab = tfidf_vectorizer.get_feature_names_out()  \n",
    "\n",
    "\n",
    "def get_weighted_glove_vector(sentence, glove_model, tfidf_vectorizer):\n",
    "    words = sentence.split()  \n",
    "    vector_size = glove_model.vector_size  \n",
    "\n",
    "    \n",
    "    tfidf_weights = {word: tfidf_vectorizer.idf_[tfidf_vectorizer.vocabulary_.get(word, 0)] for word in words}\n",
    "    \n",
    "    \n",
    "    weighted_vectors = [\n",
    "        glove_model[word] * tfidf_weights[word] for word in words if word in glove_model and word in tfidf_weights\n",
    "    ]\n",
    "    \n",
    "    if weighted_vectors:\n",
    "        return np.mean(weighted_vectors, axis=0)  \n",
    "    else:\n",
    "        return np.zeros(vector_size)  \n",
    "\n",
    "\n",
    "weighted_glove_vectors = np.array([get_weighted_glove_vector(text, glove_model, tfidf_vectorizer) for text in m1[\"cleaned_text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6da0d154-3f1e-41aa-9a87-01e8edd381ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858592</td>\n",
       "      <td>0.154181</td>\n",
       "      <td>0.323484</td>\n",
       "      <td>-0.399241</td>\n",
       "      <td>0.326458</td>\n",
       "      <td>-0.545636</td>\n",
       "      <td>-1.394057</td>\n",
       "      <td>0.281465</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>-0.542440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250909</td>\n",
       "      <td>-0.229328</td>\n",
       "      <td>-0.552093</td>\n",
       "      <td>-0.769241</td>\n",
       "      <td>0.284153</td>\n",
       "      <td>-0.017941</td>\n",
       "      <td>-0.292731</td>\n",
       "      <td>0.349016</td>\n",
       "      <td>-0.935706</td>\n",
       "      <td>0.219574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.755975</td>\n",
       "      <td>0.155494</td>\n",
       "      <td>0.010514</td>\n",
       "      <td>-0.253171</td>\n",
       "      <td>0.141417</td>\n",
       "      <td>-0.440080</td>\n",
       "      <td>-1.045392</td>\n",
       "      <td>-0.102808</td>\n",
       "      <td>-0.127356</td>\n",
       "      <td>-0.092749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.163883</td>\n",
       "      <td>-0.169573</td>\n",
       "      <td>-0.295039</td>\n",
       "      <td>-0.288185</td>\n",
       "      <td>0.236569</td>\n",
       "      <td>-0.022251</td>\n",
       "      <td>-0.365702</td>\n",
       "      <td>0.221740</td>\n",
       "      <td>-0.680323</td>\n",
       "      <td>0.331854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.906450</td>\n",
       "      <td>-0.193400</td>\n",
       "      <td>-0.554911</td>\n",
       "      <td>-0.574190</td>\n",
       "      <td>0.039106</td>\n",
       "      <td>0.255241</td>\n",
       "      <td>-1.353222</td>\n",
       "      <td>-0.320451</td>\n",
       "      <td>0.406643</td>\n",
       "      <td>0.699890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.142687</td>\n",
       "      <td>-0.292783</td>\n",
       "      <td>-0.698760</td>\n",
       "      <td>0.294014</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>-0.526897</td>\n",
       "      <td>-0.248788</td>\n",
       "      <td>0.075448</td>\n",
       "      <td>-0.827217</td>\n",
       "      <td>0.334176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.335423</td>\n",
       "      <td>1.226930</td>\n",
       "      <td>-0.094273</td>\n",
       "      <td>-0.258169</td>\n",
       "      <td>0.264507</td>\n",
       "      <td>-0.269418</td>\n",
       "      <td>-1.133745</td>\n",
       "      <td>-0.412352</td>\n",
       "      <td>0.269805</td>\n",
       "      <td>-0.626671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.831705</td>\n",
       "      <td>-0.296237</td>\n",
       "      <td>-0.094587</td>\n",
       "      <td>-0.678926</td>\n",
       "      <td>-0.059134</td>\n",
       "      <td>-0.379914</td>\n",
       "      <td>-1.164819</td>\n",
       "      <td>0.139612</td>\n",
       "      <td>-0.509723</td>\n",
       "      <td>0.853939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.679790</td>\n",
       "      <td>0.044575</td>\n",
       "      <td>-0.205409</td>\n",
       "      <td>-0.720147</td>\n",
       "      <td>0.015761</td>\n",
       "      <td>-0.410904</td>\n",
       "      <td>-1.194017</td>\n",
       "      <td>0.119634</td>\n",
       "      <td>0.407577</td>\n",
       "      <td>-0.128530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.441857</td>\n",
       "      <td>-0.001320</td>\n",
       "      <td>-0.310371</td>\n",
       "      <td>-0.686154</td>\n",
       "      <td>0.392252</td>\n",
       "      <td>0.203400</td>\n",
       "      <td>-0.652363</td>\n",
       "      <td>0.222706</td>\n",
       "      <td>-0.469796</td>\n",
       "      <td>0.349120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20229</th>\n",
       "      <td>0.797784</td>\n",
       "      <td>0.656184</td>\n",
       "      <td>-0.840474</td>\n",
       "      <td>-0.851888</td>\n",
       "      <td>0.207350</td>\n",
       "      <td>-0.137283</td>\n",
       "      <td>-1.071963</td>\n",
       "      <td>0.032974</td>\n",
       "      <td>-0.031749</td>\n",
       "      <td>-0.026924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058739</td>\n",
       "      <td>0.327891</td>\n",
       "      <td>0.077755</td>\n",
       "      <td>-0.005264</td>\n",
       "      <td>0.210573</td>\n",
       "      <td>-0.061987</td>\n",
       "      <td>-0.859505</td>\n",
       "      <td>0.567005</td>\n",
       "      <td>-0.548177</td>\n",
       "      <td>0.390130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20230</th>\n",
       "      <td>-0.285345</td>\n",
       "      <td>0.578556</td>\n",
       "      <td>-0.528137</td>\n",
       "      <td>-0.750164</td>\n",
       "      <td>0.322457</td>\n",
       "      <td>-0.179783</td>\n",
       "      <td>-0.838643</td>\n",
       "      <td>-0.364678</td>\n",
       "      <td>-0.027279</td>\n",
       "      <td>0.134014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159870</td>\n",
       "      <td>0.053914</td>\n",
       "      <td>-0.276592</td>\n",
       "      <td>-0.272625</td>\n",
       "      <td>0.104407</td>\n",
       "      <td>0.123089</td>\n",
       "      <td>-0.651834</td>\n",
       "      <td>0.355298</td>\n",
       "      <td>0.088586</td>\n",
       "      <td>0.515907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20231</th>\n",
       "      <td>0.852525</td>\n",
       "      <td>-0.112452</td>\n",
       "      <td>-0.129401</td>\n",
       "      <td>-0.009184</td>\n",
       "      <td>0.069486</td>\n",
       "      <td>-0.274017</td>\n",
       "      <td>-0.739938</td>\n",
       "      <td>-0.486598</td>\n",
       "      <td>-0.387123</td>\n",
       "      <td>-0.049008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>-0.019202</td>\n",
       "      <td>-0.442865</td>\n",
       "      <td>-0.034064</td>\n",
       "      <td>0.475864</td>\n",
       "      <td>-0.074331</td>\n",
       "      <td>-0.675853</td>\n",
       "      <td>0.447019</td>\n",
       "      <td>-0.687754</td>\n",
       "      <td>0.399706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20232</th>\n",
       "      <td>0.306964</td>\n",
       "      <td>0.263512</td>\n",
       "      <td>-0.267312</td>\n",
       "      <td>-0.562546</td>\n",
       "      <td>-0.228523</td>\n",
       "      <td>-0.699757</td>\n",
       "      <td>-0.474766</td>\n",
       "      <td>-0.097210</td>\n",
       "      <td>-0.234700</td>\n",
       "      <td>-0.485795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313377</td>\n",
       "      <td>0.191517</td>\n",
       "      <td>-0.490260</td>\n",
       "      <td>-0.014799</td>\n",
       "      <td>-0.065914</td>\n",
       "      <td>0.716367</td>\n",
       "      <td>-1.164535</td>\n",
       "      <td>0.232941</td>\n",
       "      <td>-0.223261</td>\n",
       "      <td>0.847906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20233</th>\n",
       "      <td>0.571358</td>\n",
       "      <td>0.044364</td>\n",
       "      <td>-0.210265</td>\n",
       "      <td>-0.201443</td>\n",
       "      <td>0.464685</td>\n",
       "      <td>-0.654891</td>\n",
       "      <td>-0.946773</td>\n",
       "      <td>-0.059428</td>\n",
       "      <td>-0.083986</td>\n",
       "      <td>-0.452680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054497</td>\n",
       "      <td>0.284489</td>\n",
       "      <td>-0.340743</td>\n",
       "      <td>-0.797162</td>\n",
       "      <td>0.018406</td>\n",
       "      <td>0.060033</td>\n",
       "      <td>-0.504586</td>\n",
       "      <td>0.175298</td>\n",
       "      <td>-0.336184</td>\n",
       "      <td>-0.076625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20234 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.858592  0.154181  0.323484 -0.399241  0.326458 -0.545636 -1.394057   \n",
       "1      0.755975  0.155494  0.010514 -0.253171  0.141417 -0.440080 -1.045392   \n",
       "2      0.906450 -0.193400 -0.554911 -0.574190  0.039106  0.255241 -1.353222   \n",
       "3      0.335423  1.226930 -0.094273 -0.258169  0.264507 -0.269418 -1.133745   \n",
       "4      0.679790  0.044575 -0.205409 -0.720147  0.015761 -0.410904 -1.194017   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "20229  0.797784  0.656184 -0.840474 -0.851888  0.207350 -0.137283 -1.071963   \n",
       "20230 -0.285345  0.578556 -0.528137 -0.750164  0.322457 -0.179783 -0.838643   \n",
       "20231  0.852525 -0.112452 -0.129401 -0.009184  0.069486 -0.274017 -0.739938   \n",
       "20232  0.306964  0.263512 -0.267312 -0.562546 -0.228523 -0.699757 -0.474766   \n",
       "20233  0.571358  0.044364 -0.210265 -0.201443  0.464685 -0.654891 -0.946773   \n",
       "\n",
       "            7         8         9    ...       190       191       192  \\\n",
       "0      0.281465 -0.123213 -0.542440  ... -0.250909 -0.229328 -0.552093   \n",
       "1     -0.102808 -0.127356 -0.092749  ... -0.163883 -0.169573 -0.295039   \n",
       "2     -0.320451  0.406643  0.699890  ... -0.142687 -0.292783 -0.698760   \n",
       "3     -0.412352  0.269805 -0.626671  ... -0.831705 -0.296237 -0.094587   \n",
       "4      0.119634  0.407577 -0.128530  ... -0.441857 -0.001320 -0.310371   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "20229  0.032974 -0.031749 -0.026924  ... -0.058739  0.327891  0.077755   \n",
       "20230 -0.364678 -0.027279  0.134014  ... -0.159870  0.053914 -0.276592   \n",
       "20231 -0.486598 -0.387123 -0.049008  ...  0.002466 -0.019202 -0.442865   \n",
       "20232 -0.097210 -0.234700 -0.485795  ...  0.313377  0.191517 -0.490260   \n",
       "20233 -0.059428 -0.083986 -0.452680  ...  0.054497  0.284489 -0.340743   \n",
       "\n",
       "            193       194       195       196       197       198       199  \n",
       "0     -0.769241  0.284153 -0.017941 -0.292731  0.349016 -0.935706  0.219574  \n",
       "1     -0.288185  0.236569 -0.022251 -0.365702  0.221740 -0.680323  0.331854  \n",
       "2      0.294014  0.573864 -0.526897 -0.248788  0.075448 -0.827217  0.334176  \n",
       "3     -0.678926 -0.059134 -0.379914 -1.164819  0.139612 -0.509723  0.853939  \n",
       "4     -0.686154  0.392252  0.203400 -0.652363  0.222706 -0.469796  0.349120  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "20229 -0.005264  0.210573 -0.061987 -0.859505  0.567005 -0.548177  0.390130  \n",
       "20230 -0.272625  0.104407  0.123089 -0.651834  0.355298  0.088586  0.515907  \n",
       "20231 -0.034064  0.475864 -0.074331 -0.675853  0.447019 -0.687754  0.399706  \n",
       "20232 -0.014799 -0.065914  0.716367 -1.164535  0.232941 -0.223261  0.847906  \n",
       "20233 -0.797162  0.018406  0.060033 -0.504586  0.175298 -0.336184 -0.076625  \n",
       "\n",
       "[20234 rows x 200 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_df = pd.DataFrame(weighted_glove_vectors)\n",
    "glove_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94fbb7ec-d61d-4a47-8d93-9ceb99b31c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tfidf_df = pd.read_parquet(\"tfidf_yelp_reviews.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14812c7-1a52-48f8-b0d7-326433ecc00e",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cee958e4-c440-444f-8896-1c354ec18838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1741: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "  chicken dish sauce fried restaurant shrimp good food rice meal\n",
      "--------------------------------------------------\n",
      "Topic #1:\n",
      "  pizza crust pie slice cheese pepperoni topping thin sauce delivery\n",
      "--------------------------------------------------\n",
      "Topic #2:\n",
      "  coffee donut breakfast shop tea latte egg cup iced toast\n",
      "--------------------------------------------------\n",
      "Topic #3:\n",
      "  ice cream chocolate cake flavor cooky cone scoop sweet dessert\n",
      "--------------------------------------------------\n",
      "Topic #4:\n",
      "  burger fry bacon bun onion patty cheese shake good got\n",
      "--------------------------------------------------\n",
      "Topic #5:\n",
      "  bar beer wine great drink cocktail night table service area\n",
      "--------------------------------------------------\n",
      "Topic #6:\n",
      "  taco salsa mexican burrito chip tortilla bean asada food margarita\n",
      "--------------------------------------------------\n",
      "Topic #7:\n",
      "  sandwich cheese bread bagel breakfast cheesesteak salad lunch steak philly\n",
      "--------------------------------------------------\n",
      "Topic #8:\n",
      "  sushi roll tuna fish lunch tempura salmon fresh rice soup\n",
      "--------------------------------------------------\n",
      "Topic #9:\n",
      "  food get place like time one go would know order\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'original_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 41\u001b[0m\n\u001b[1;32m     36\u001b[0m doc_topics \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(W, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# doc_topics[i] = the most dominant topic for document i\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# If you want to add this back to a DataFrame with your original data:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# (Assuming you have a matching index of documents)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43moriginal_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdominant_topic\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m doc_topics\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original_df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "tfidf_df = pd.read_parquet(\"tfidf_yelp_reviews.parquet\")\n",
    "\n",
    "tfidf_df.shape  \n",
    "\n",
    "tfidf_matrix = tfidf_df.values\n",
    "\n",
    "num_topics = 10  \n",
    "nmf_model = NMF(\n",
    "    n_components=num_topics,\n",
    "    init='nndsvd',     \n",
    "    random_state=42,\n",
    "    max_iter=200       \n",
    ")\n",
    "\n",
    "W = nmf_model.fit_transform(tfidf_matrix)\n",
    "H = nmf_model.components_\n",
    "\n",
    "feature_names = tfidf_df.columns  \n",
    "n_top_words = 10\n",
    "\n",
    "for topic_idx, topic_weights in enumerate(H):\n",
    "    top_word_indices = topic_weights.argsort()[::-1][:n_top_words]\n",
    "    top_words = [feature_names[i] for i in top_word_indices]\n",
    "    print(f\"Topic #{topic_idx}:\")\n",
    "    print(\"  \" + \" \".join(top_words))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "doc_topics = np.argmax(W, axis=1)\n",
    "\n",
    "original_df['dominant_topic'] = doc_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2102cdfb-7da1-408a-85d5-9c74d1dc3274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Topics: 2, Reconstruction Error: 139.13758544380636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1741: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Topics: 3, Reconstruction Error: 138.8253897853609\n",
      "Number of Topics: 4, Reconstruction Error: 138.55638377103676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1741: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Topics: 5, Reconstruction Error: 138.33369454630682\n",
      "Number of Topics: 6, Reconstruction Error: 138.10742276539472\n",
      "Number of Topics: 7, Reconstruction Error: 137.88177114492396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1741: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Topics: 8, Reconstruction Error: 137.69726906561257\n",
      "Number of Topics: 9, Reconstruction Error: 137.48677950283223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1741: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Topics: 10, Reconstruction Error: 137.30799559329037\n",
      "Number of Topics: 11, Reconstruction Error: 137.15289177730807\n",
      "Number of Topics: 12, Reconstruction Error: 136.96502121325926\n",
      "Number of Topics: 13, Reconstruction Error: 136.7895859144265\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m nmf_model \u001b[38;5;241m=\u001b[39m NMF(\n\u001b[1;32m     15\u001b[0m     n_components\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m     16\u001b[0m     init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnndsvd\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     18\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mnmf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Store the reconstruction error\u001b[39;00m\n\u001b[1;32m     23\u001b[0m reconstruction_errors\u001b[38;5;241m.\u001b[39mappend(nmf_model\u001b[38;5;241m.\u001b[39mreconstruction_err_)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1296\u001b[0m, in \u001b[0;36m_BaseNMF.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Learn a NMF model for the data X.\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \n\u001b[1;32m   1276\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;124;03m    Returns the instance itself.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;66;03m# param validation is done in fit_transform\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1639\u001b[0m, in \u001b[0;36mNMF.fit_transform\u001b[0;34m(self, X, y, W, H)\u001b[0m\n\u001b[1;32m   1634\u001b[0m X \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m   1636\u001b[0m )\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(assume_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 1639\u001b[0m     W, H, n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstruction_err_ \u001b[38;5;241m=\u001b[39m _beta_divergence(\n\u001b[1;32m   1642\u001b[0m     X, W, H, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beta_loss, square_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1643\u001b[0m )\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_ \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1707\u001b[0m, in \u001b[0;36mNMF._fit_transform\u001b[0;34m(self, X, y, W, H, update_H)\u001b[0m\n\u001b[1;32m   1704\u001b[0m l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_regularization(X)\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1707\u001b[0m     W, H, n_iter \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_coordinate_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_reg_W\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_reg_H\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml2_reg_W\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml2_reg_H\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupdate_H\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_H\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1723\u001b[0m     W, H, n_iter, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m _fit_multiplicative_update(\n\u001b[1;32m   1724\u001b[0m         X,\n\u001b[1;32m   1725\u001b[0m         W,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   1736\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:506\u001b[0m, in \u001b[0;36m_fit_coordinate_descent\u001b[0;34m(X, W, H, tol, max_iter, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, verbose, shuffle, random_state)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Update H\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_H:\n\u001b[0;32m--> 506\u001b[0m     violation \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m_update_coordinate_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_reg_H\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_H\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    511\u001b[0m     violation_init \u001b[38;5;241m=\u001b[39m violation\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:388\u001b[0m, in \u001b[0;36m_update_coordinate_descent\u001b[0;34m(X, W, Ht, l1_reg, l2_reg, shuffle, random_state)\u001b[0m\n\u001b[1;32m    385\u001b[0m n_components \u001b[38;5;241m=\u001b[39m Ht\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    387\u001b[0m HHt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Ht\u001b[38;5;241m.\u001b[39mT, Ht)\n\u001b[0;32m--> 388\u001b[0m XHt \u001b[38;5;241m=\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# L2 regularization corresponds to increase of the diagonal of HHt\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l2_reg \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# adds l2_reg only on the diagonal\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/extmath.py:206\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 206\u001b[0m     \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m ):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/sparse/_base.py:1461\u001b[0m, in \u001b[0;36missparse\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m sparray\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m _spbase\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m-> 1461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missparse\u001b[39m(x):\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is `x` of a sparse array type?\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m \n\u001b[1;32m   1464\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _spbase)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "tfidf_matrix = tfidf_df.values\n",
    "\n",
    "\n",
    "topic_range = range(2, 21)  \n",
    "reconstruction_errors = []\n",
    "\n",
    "for k in topic_range:\n",
    "    nmf_model = NMF(\n",
    "        n_components=k,\n",
    "        init='nndsvd',\n",
    "        random_state=42,\n",
    "        max_iter=200\n",
    "    )\n",
    "    \n",
    "    nmf_model.fit(tfidf_matrix)\n",
    "    \n",
    "    reconstruction_errors.append(nmf_model.reconstruction_err_)\n",
    "    print(f\"Number of Topics: {k}, Reconstruction Error: {nmf_model.reconstruction_err_}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(topic_range, reconstruction_errors, marker='o')\n",
    "plt.xlabel(\"Number of Topics (k)\")\n",
    "plt.ylabel(\"Reconstruction Error\")\n",
    "plt.title(\"NMF Reconstruction Error vs. Number of Topics\")\n",
    "plt.xticks(list(topic_range))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e56e11f-1cf8-4375-89d8-5c39b5aff26a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1741: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated sentiment by topic (approximate):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>positive_count</th>\n",
       "      <th>negative_count</th>\n",
       "      <th>neutral_count</th>\n",
       "      <th>total_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.320135</td>\n",
       "      <td>3245</td>\n",
       "      <td>52</td>\n",
       "      <td>146</td>\n",
       "      <td>3443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.253941</td>\n",
       "      <td>894</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.293761</td>\n",
       "      <td>1455</td>\n",
       "      <td>53</td>\n",
       "      <td>73</td>\n",
       "      <td>1581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.294136</td>\n",
       "      <td>1017</td>\n",
       "      <td>22</td>\n",
       "      <td>41</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.269202</td>\n",
       "      <td>802</td>\n",
       "      <td>40</td>\n",
       "      <td>59</td>\n",
       "      <td>901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.347354</td>\n",
       "      <td>2467</td>\n",
       "      <td>41</td>\n",
       "      <td>83</td>\n",
       "      <td>2591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.278107</td>\n",
       "      <td>812</td>\n",
       "      <td>20</td>\n",
       "      <td>44</td>\n",
       "      <td>876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.257022</td>\n",
       "      <td>1302</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.293754</td>\n",
       "      <td>712</td>\n",
       "      <td>14</td>\n",
       "      <td>39</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.226484</td>\n",
       "      <td>5331</td>\n",
       "      <td>645</td>\n",
       "      <td>558</td>\n",
       "      <td>6534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dominant_topic  avg_sentiment  positive_count  negative_count  \\\n",
       "0               0       0.320135            3245              52   \n",
       "1               1       0.253941             894              57   \n",
       "2               2       0.293761            1455              53   \n",
       "3               3       0.294136            1017              22   \n",
       "4               4       0.269202             802              40   \n",
       "5               5       0.347354            2467              41   \n",
       "6               6       0.278107             812              20   \n",
       "7               7       0.257022            1302              52   \n",
       "8               8       0.293754             712              14   \n",
       "9               9       0.226484            5331             645   \n",
       "\n",
       "   neutral_count  total_docs  \n",
       "0            146        3443  \n",
       "1             54        1005  \n",
       "2             73        1581  \n",
       "3             41        1080  \n",
       "4             59         901  \n",
       "5             83        2591  \n",
       "6             44         876  \n",
       "7            104        1458  \n",
       "8             39         765  \n",
       "9            558        6534  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.714842</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.379902</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.233411</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.311422</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.543250</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dominant_topic  sentiment_score sentiment_label\n",
       "0               0         0.714842        POSITIVE\n",
       "1               0         0.379902        POSITIVE\n",
       "2               8        -0.233411        NEGATIVE\n",
       "3               2         0.311422        POSITIVE\n",
       "4               5         0.543250        POSITIVE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "tfidf_df = pd.read_parquet(\"tfidf_yelp_reviews.parquet\")\n",
    "tfidf_matrix = tfidf_df.values\n",
    "\n",
    "num_topics = 10\n",
    "nmf_model = NMF(n_components=num_topics, init='nndsvd', random_state=42, max_iter=200)\n",
    "W = nmf_model.fit_transform(tfidf_matrix)\n",
    "H = nmf_model.components_\n",
    "\n",
    "\n",
    "doc_topics = np.argmax(W, axis=1)\n",
    "tfidf_df['dominant_topic'] = doc_topics\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "col_sentiment = {}\n",
    "for col in tfidf_df.columns:\n",
    "    \n",
    "    if col == 'dominant_topic':\n",
    "        col_sentiment[col] = 0.0\n",
    "        continue\n",
    "\n",
    "\n",
    "    if not isinstance(col, str) or not col.strip():\n",
    "        col_sentiment[col] = 0.0\n",
    "    else:\n",
    "        score = sid.polarity_scores(col)['compound']\n",
    "        col_sentiment[col] = score\n",
    "\n",
    "\n",
    "def compute_row_sentiment(row):\n",
    "    total_score = 0.0\n",
    "    for word, tfidf_val in row.items():\n",
    "        total_score += tfidf_val * col_sentiment.get(word, 0.0)\n",
    "    return total_score\n",
    "\n",
    "\n",
    "tfidf_df['sentiment_score'] = tfidf_df.drop(columns='dominant_topic').apply(\n",
    "    compute_row_sentiment, axis=1\n",
    ")\n",
    "\n",
    "\n",
    "def label_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'POSITIVE'\n",
    "    elif score <= -0.05:\n",
    "        return 'NEGATIVE'\n",
    "    else:\n",
    "        return 'NEUTRAL'\n",
    "\n",
    "tfidf_df['sentiment_label'] = tfidf_df['sentiment_score'].apply(label_sentiment)\n",
    "\n",
    "\n",
    "topic_sentiment = tfidf_df.groupby('dominant_topic').agg(\n",
    "    avg_sentiment=('sentiment_score', 'mean'),\n",
    "    positive_count=('sentiment_label', lambda x: (x == 'POSITIVE').sum()),\n",
    "    negative_count=('sentiment_label', lambda x: (x == 'NEGATIVE').sum()),\n",
    "    neutral_count=('sentiment_label', lambda x: (x == 'NEUTRAL').sum()),\n",
    "    total_docs=('sentiment_label', 'count')\n",
    ").reset_index()\n",
    "\n",
    "print(\"Aggregated sentiment by topic (approximate):\")\n",
    "display(topic_sentiment)\n",
    "\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "display(tfidf_df[['dominant_topic', 'sentiment_score', 'sentiment_label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f8fba59-b642-4232-ab7b-8d51f68e98a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a keyword for restaurant search (e.g., 'pizza', 'burger', 'sushi'):  pizza\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dominant_topic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dominant_topic'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 66\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# 4. Usage: Prompt the User for a Keyword and Display Recommendations\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m     65\u001b[0m user_keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a keyword for restaurant search (e.g., \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpizza\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mburger\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msushi\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 66\u001b[0m recs \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend_restaurants\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_keyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_review\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recs\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo recommendations found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m, in \u001b[0;36mrecommend_restaurants\u001b[0;34m(keyword, df, top_n)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Filter reviews whose dominant_topic is among the matched topics\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m subset \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdominant_topic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misin(matched_topics)]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subset\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo reviews found for topics related to keyword: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dominant_topic'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "\n",
    "topic_to_top_words = {}\n",
    "for topic_idx in range(num_topics):\n",
    "    topic_weights = H[topic_idx]\n",
    "    top_indices = topic_weights.argsort()[::-1][:n_top_words]\n",
    "    topic_to_top_words[topic_idx] = [feature_names[i] for i in top_indices]\n",
    "\n",
    "\n",
    "def find_topics_for_keyword(keyword):\n",
    "    keyword_lower = keyword.lower()\n",
    "    matched_topics = []\n",
    "    for topic, words in topic_to_top_words.items():\n",
    "        \n",
    "        if keyword_lower in [w.lower() for w in words]:\n",
    "            matched_topics.append(topic)\n",
    "    return matched_topics\n",
    "\n",
    "\n",
    "def recommend_restaurants(keyword, df, top_n=5):\n",
    "    # Find topics whose top words contain the keyword\n",
    "    matched_topics = find_topics_for_keyword(keyword)\n",
    "    if not matched_topics:\n",
    "        print(f\"No topics found matching keyword: '{keyword}'\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter reviews whose dominant_topic is among the matched topics\n",
    "    subset = df[df['dominant_topic'].isin(matched_topics)]\n",
    "    if subset.empty:\n",
    "        print(f\"No reviews found for topics related to keyword: '{keyword}'\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Group by restaurant (business_id) and aggregate:\n",
    "    # Use the first occurrence of \"name\" and \"stars\" (assumed consistent per restaurant),\n",
    "    # and compute average sentiment and review count.\n",
    "    recommendations = subset.groupby('business_id').agg(\n",
    "        name=('name', 'first'),\n",
    "        stars=('stars', 'first'),\n",
    "        avg_sentiment=('sentiment_score', 'mean'),\n",
    "        review_count=('sentiment_score', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Sort by average sentiment (highest first)\n",
    "    recommendations = recommendations.sort_values('avg_sentiment', ascending=False)\n",
    "    return recommendations.head(top_n)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Usage: Prompt the User for a Keyword and Display Recommendations\n",
    "# -------------------------\n",
    "user_keyword = input(\"Enter a keyword for restaurant search (e.g., 'pizza', 'burger', 'sushi'): \").strip()\n",
    "recs = recommend_restaurants(user_keyword, merged_review, top_n=10)\n",
    "\n",
    "if recs.empty:\n",
    "    print(\"No recommendations found.\")\n",
    "else:\n",
    "    print(f\"\\nTop restaurant recommendations for keyword '{user_keyword}':\")\n",
    "    print(recs[['name', 'stars', 'avg_sentiment', 'review_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc2bd0-fa1a-4e2b-a61c-d265f470e808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-17.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-17:m127"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
